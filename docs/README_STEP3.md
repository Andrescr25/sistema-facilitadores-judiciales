# PASO 3 - Pipeline de recuperaci√≥n RAG y test en consola

## Objetivo
Crear un chat console que integre la base de datos vectorial con GPT4All para generar respuestas contextuales sobre facilitaci√≥n judicial.

## Archivos generados
- `chat_console.py` - Script principal del chat con RAG
- `config.env` - Archivo de configuraci√≥n con variables de entorno
- `README_STEP3.md` - Este archivo de instrucciones

## Caracter√≠sticas del chat console

### Funcionalidades principales:
- ‚úÖ Integraci√≥n completa con RAG (Retrieval Augmented Generation)
- ‚úÖ B√∫squeda sem√°ntica en la base de datos vectorial
- ‚úÖ Respuestas contextuales basadas en documentos
- ‚úÖ Muestra fuentes consultadas
- ‚úÖ Prompt template especializado para facilitadores judiciales
- ‚úÖ Pruebas autom√°ticas con consultas de ejemplo
- ‚úÖ Modo interactivo para preguntas personalizadas

### Prompt template especializado:
El bot est√° configurado con un prompt espec√≠fico que:
- Se enfoca en facilitaci√≥n judicial y resoluci√≥n de conflictos
- Usa √∫nicamente informaci√≥n de los documentos
- Mantiene un tono profesional y formal
- Menciona fuentes cuando es relevante
- Indica claramente cuando no tiene informaci√≥n suficiente

## Instrucciones de ejecuci√≥n

### 1. Configurar variables de entorno
```bash
# Opci√≥n 1: Usar archivo de configuraci√≥n
source config.env

# Opci√≥n 2: Configurar manualmente
export MODEL_PATH="/ruta/a/tu/modelo.gguf"
export VECTOR_DB_DIR="./data/chroma"
```

### 2. Activar entorno virtual
```bash
source venv/bin/activate
```

### 3. Ejecutar chat console
```bash
python chat_console.py
```

## Salida esperada

El script mostrar√°:
```
ü§ñ Bot de Facilitadores Judiciales - Chat Console
============================================================

2024-01-XX XX:XX:XX - INFO - üöÄ Inicializando bot de Facilitadores Judiciales...
2024-01-XX XX:XX:XX - INFO - Cargando base de datos vectorial...
2024-01-XX XX:XX:XX - INFO - ‚úÖ Base de datos cargada con 7 documentos
2024-01-XX XX:XX:XX - INFO - Cargando modelo de lenguaje...
2024-01-XX XX:XX:XX - INFO - ‚úÖ Modelo cargado con LangChain
2024-01-XX XX:XX:XX - INFO - Creando cadena de pregunta-respuesta...
2024-01-XX XX:XX:XX - INFO - ‚úÖ Cadena de QA creada exitosamente
2024-01-XX XX:XX:XX - INFO - ‚úÖ Bot inicializado correctamente

üß™ Ejecutando pruebas autom√°ticas...
============================================================

--- PRUEBA 1 ---
Pregunta: ¬øCu√°les son los requisitos para ser facilitador judicial?
Respuesta: [Respuesta basada en los documentos]
Fuentes:
  1. marco_legal.txt
  2. guia_facilitador.txt
--------------------------------------------------

üí¨ Modo interactivo (escribe 'exit' para salir)
============================================================

ü§î Pregunta: [Tu pregunta aqu√≠]
```

## Pruebas autom√°ticas incluidas

El script ejecuta autom√°ticamente estas consultas de prueba:

1. **"¬øCu√°les son los requisitos para ser facilitador judicial?"**
   - Deber√≠a devolver informaci√≥n sobre t√≠tulo profesional, experiencia, certificaci√≥n, etc.

2. **"¬øCu√°nto dura el procedimiento de conciliaci√≥n?"**
   - Deber√≠a mencionar los 30 d√≠as h√°biles seg√∫n el documento

3. **"¬øQu√© t√©cnicas de facilitaci√≥n se recomiendan?"**
   - Deber√≠a listar escucha activa, parafraseo, preguntas abiertas, etc.

4. **"¬øCu√°les son los costos del procedimiento?"**
   - Deber√≠a mostrar las tasas y honorarios espec√≠ficos

## Soluci√≥n de problemas

### Error: "Modelo no encontrado"
```bash
# Verificar que el modelo existe
ls -la /ruta/a/tu/modelo.gguf

# Actualizar la ruta en config.env
export MODEL_PATH="/ruta/correcta/a/tu/modelo.gguf"
```

### Error: "Base de datos no encontrada"
```bash
# Ejecutar primero la ingesta
python ingest.py
```

### Error: "No module named 'gpt4all'"
```bash
# Reinstalar gpt4all
pip uninstall gpt4all
pip install gpt4all==2.0.0
```

### Error de memoria al cargar el modelo
```bash
# Usar un modelo m√°s peque√±o o aumentar la memoria virtual
# En macOS, el sistema maneja esto autom√°ticamente
```

## Comandos √∫tiles

### Verificar configuraci√≥n
```bash
# Verificar variables de entorno
env | grep -E "(MODEL_PATH|VECTOR_DB_DIR)"

# Verificar que existe la base de datos
ls -la data/chroma/
```

### Probar consultas espec√≠ficas
```python
# Script de prueba r√°pida
from chat_console import FacilitadorBot

bot = FacilitadorBot(MODEL_PATH, VECTOR_DB_DIR)
if bot.initialize():
    result = bot.ask("¬øCu√°les son las fases del procedimiento?")
    print(result['answer'])
```

## Personalizaci√≥n

### Cambiar el prompt template
Edita la variable `FACILITADOR_PROMPT_TEMPLATE` en `chat_console.py` para:
- Modificar el tono de las respuestas
- Agregar instrucciones espec√≠ficas
- Cambiar el formato de salida

### Ajustar par√°metros de b√∫squeda
Modifica en `create_qa_chain()`:
```python
retriever = self.vectordb.as_retriever(
    search_kwargs={"k": 5}  # M√°s documentos para consultar
)
```

### Cambiar configuraci√≥n del modelo
Modifica en `load_llm()`:
```python
self.llm = GPT4AllLangChain(
    model=self.model_path,
    n_ctx=4096,      # Contexto m√°s grande
    temperature=0.2,  # M√°s creatividad
    max_tokens=1024   # Respuestas m√°s largas
)
```

## Pr√≥ximos pasos

Una vez completado este paso:
1. Verifica que las pruebas autom√°ticas funcionan correctamente
2. Prueba el modo interactivo con tus propias preguntas
3. Escribe `continuar` para proceder al **PASO 4** (API backend con FastAPI)

## Notas importantes

- ‚ö†Ô∏è El modelo GPT4All debe estar descargado y en la ruta correcta
- ‚úÖ El sistema usa MPS (Metal Performance Shaders) en macOS para aceleraci√≥n
- ‚úÖ Las respuestas se basan √∫nicamente en los documentos procesados
- ‚úÖ El sistema muestra las fuentes consultadas para transparencia
